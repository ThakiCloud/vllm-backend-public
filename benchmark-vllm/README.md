# Benchmark vLLM Service

FastAPI ê¸°ë°˜ vLLM ë°°í¬ ë° ê´€ë¦¬ ì„œë¹„ìŠ¤ì…ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” ê¸°ëŠ¥

- YAML íŒŒì¼ì„ í†µí•œ vLLM ì„¤ì • ê´€ë¦¬
- Kubernetes í´ëŸ¬ìŠ¤í„°ì— vLLM ì„œë²„ ë°°í¬
- REST APIë¥¼ í†µí•œ vLLM ì„œë²„ ë°°í¬ ë° ê´€ë¦¬
- ì‹¤ì‹œê°„ ë°°í¬ ìƒíƒœ ëª¨ë‹ˆí„°ë§
- ë‹¤ì¤‘ ëª¨ë¸ ë™ì‹œ ë°°í¬ ì§€ì›
- vllm ë„¤ì„ìŠ¤í˜ì´ìŠ¤ì—ì„œ Pod ê²©ë¦¬ ì‹¤í–‰
- **GPU ë¦¬ì†ŒìŠ¤ ì¶©ëŒ ê°ì§€ ë° ìë™ í•´ê²°**
- **ë™ì¼í•œ ì„¤ì •ì˜ ë°°í¬ ì¬ì‚¬ìš©**
- **MIG GPU ë¦¬ì†ŒìŠ¤ ì§€ì›**

## ğŸš€ ë¡œì»¬ ì‹¤í–‰

### 1. ì˜ì¡´ì„± ì„¤ì¹˜

```bash
pip install -r requirements.txt
```

### 2. í™˜ê²½ ë³€ìˆ˜ ì„¤ì •

```bash
export MONGO_URL="mongodb://localhost:27017/?directConnection=true&serverSelectionTimeoutMS=5000&connectTimeoutMS=5000"
export SERVER_PORT=8005
export VLLM_CONFIG_DIR="./configs"
export VLLM_NAMESPACE="vllm"
export KUBECONFIG="/path/to/your/kubeconfig"
```

### 3. ì„œë¹„ìŠ¤ ì‹¤í–‰

```bash
python main.py
```

## ğŸ MacBook í™˜ê²½ì—ì„œ í…ŒìŠ¤íŠ¸

MacBookì—ì„œ ì „ì²´ ê¸°ëŠ¥ì„ í…ŒìŠ¤íŠ¸í•˜ë ¤ë©´ ë¡œì»¬ Kubernetes í´ëŸ¬ìŠ¤í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤.

### ì˜µì…˜ 1: Docker Desktop with Kubernetes (ê¶Œì¥)

1. **Docker Desktop ì„¤ì¹˜ ë° Kubernetes í™œì„±í™”**
   ```bash
   # Docker Desktop ì„¤ì¹˜ í›„ Settings > Kubernetes > Enable Kubernetes ì²´í¬
   ```

2. **MongoDB ë¡œì»¬ ì‹¤í–‰**
   ```bash
   # Dockerë¡œ MongoDB ì‹¤í–‰
   docker run -d --name mongodb -p 27017:27017 mongo:latest
   
   # ë˜ëŠ” brewë¡œ ì„¤ì¹˜
   brew install mongodb-community
   brew services start mongodb-community
   ```

3. **ê°€ìƒí™˜ê²½ ì„¤ì • ë° ì‹¤í–‰**
   ```bash
   # ê°€ìƒí™˜ê²½ ìƒì„± ë° í™œì„±í™”
   python -m venv venv
   source venv/bin/activate
   
   # ì˜ì¡´ì„± ì„¤ì¹˜
   pip install -r requirements.txt
   
   # í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
   export MONGO_URL="mongodb://localhost:27017/?directConnection=true&serverSelectionTimeoutMS=5000&connectTimeoutMS=5000"
   export SERVER_PORT=8005
   export VLLM_CONFIG_DIR="./configs"
   export VLLM_NAMESPACE="vllm"
   export KUBECONFIG="$HOME/.kube/config"
   
   # ì„œë¹„ìŠ¤ ì‹¤í–‰
   python main.py
   ```

4. **Kubernetes ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ìƒì„±**
   ```bash
   kubectl create namespace vllm
   ```

### ì˜µì…˜ 2: Minikube ì‚¬ìš©

1. **Minikube ì„¤ì¹˜ ë° ì‹œì‘**
   ```bash
   # Minikube ì„¤ì¹˜
   brew install minikube
   
   # Minikube ì‹œì‘ (GPU ì§€ì› ì—†ìŒ)
   minikube start --driver=docker
   
   # Kubernetes ì»¨í…ìŠ¤íŠ¸ í™•ì¸
   kubectl config current-context
   ```

2. **ë‚˜ë¨¸ì§€ ì„¤ì •ì€ ì˜µì…˜ 1ê³¼ ë™ì¼**

### ì˜µì…˜ 3: Kind (Kubernetes in Docker) ì‚¬ìš©

1. **Kind ì„¤ì¹˜ ë° í´ëŸ¬ìŠ¤í„° ìƒì„±**
   ```bash
   # Kind ì„¤ì¹˜
   brew install kind
   
   # í´ëŸ¬ìŠ¤í„° ìƒì„±
   kind create cluster --name vllm-test
   
   # ì»¨í…ìŠ¤íŠ¸ ì„¤ì •
   kubectl cluster-info --context kind-vllm-test
   ```

2. **ë‚˜ë¨¸ì§€ ì„¤ì •ì€ ì˜µì…˜ 1ê³¼ ë™ì¼**

### ğŸ§ª MacBookì—ì„œ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸

#### 1. ì„¤ì • ë§¤ì¹­ í…ŒìŠ¤íŠ¸ (Kubernetes ì—†ì´ ê°€ëŠ¥)
```bash
# ê°€ìƒí™˜ê²½ í™œì„±í™”
source venv/bin/activate

# ì„¤ì • ë§¤ì¹­ ë¡œì§ë§Œ í…ŒìŠ¤íŠ¸
python -c "
import asyncio
from models import VLLMConfig

async def test_config_matching():
    config1 = VLLMConfig(
        model_name='microsoft/DialoGPT-medium',
        gpu_resource_type='nvidia.com/gpu',
        gpu_resource_count=1
    )
    
    config2 = VLLMConfig(
        model_name='microsoft/DialoGPT-medium',
        gpu_resource_type='nvidia.com/gpu',
        gpu_resource_count=1
    )
    
    print(f'Config matching: {config1.matches_config(config2)}')
    print(f'GPU conflict: {config1.conflicts_with_gpu_resources(config2)}')

asyncio.run(test_config_matching())
"
```

#### 2. API í…ŒìŠ¤íŠ¸ (MongoDB + Kubernetes í•„ìš”)
```bash
# ì„œë¹„ìŠ¤ ì‹¤í–‰ í›„ ë‹¤ë¥¸ í„°ë¯¸ë„ì—ì„œ
curl -X POST http://localhost:8005/deploy \
  -H "Content-Type: application/json" \
  -d '{
    "config": {
      "model_name": "microsoft/DialoGPT-medium",
      "gpu_resource_type": "nvidia.com/gpu",
      "gpu_resource_count": 1,
      "port": 8000
    }
  }'
```

#### 3. ë°°í¬ ìƒíƒœ í™•ì¸
```bash
# ë°°í¬ ëª©ë¡ í™•ì¸
curl http://localhost:8005/deployments

# Kubernetes ë¦¬ì†ŒìŠ¤ í™•ì¸
kubectl get pods -n vllm
kubectl get deployments -n vllm
kubectl get services -n vllm
```

## ğŸš€ MacBook í…ŒìŠ¤íŠ¸ ëª…ë ¹ì–´ ëª¨ìŒ

### ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ ì‚¬ìš©

```bash
# ì‹¤í–‰ ê¶Œí•œ ë¶€ì—¬
chmod +x test_deployment_comparison.sh

# ì „ì²´ ë°°í¬ ë¹„êµ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
./test_deployment_comparison.sh test

# í˜„ì¬ ë°°í¬ ëª©ë¡ í™•ì¸
./test_deployment_comparison.sh list

# íŠ¹ì • ì„¤ì •ìœ¼ë¡œ ë°°í¬
./test_deployment_comparison.sh deploy test_scenario1.yaml

# ëª¨ë“  ë°°í¬ ì¤‘ì§€
./test_deployment_comparison.sh stop-all

# ì„œë²„ ìƒíƒœ í™•ì¸
./test_deployment_comparison.sh health

# ë„ì›€ë§ ë³´ê¸°
./test_deployment_comparison.sh help
```

### ìˆ˜ë™ í…ŒìŠ¤íŠ¸ ëª…ë ¹ì–´

#### 1. ì„œë²„ ìƒíƒœ í™•ì¸
```bash
# ì„œë²„ í—¬ìŠ¤ ì²´í¬
curl http://localhost:8005/health

# ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸
curl http://localhost:8005/status

# ì„¤ì • íŒŒì¼ ëª©ë¡ í™•ì¸
curl http://localhost:8005/configs/files
```

#### 2. ë°°í¬ ë¹„êµ í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤

**ì‹œë‚˜ë¦¬ì˜¤ 1: ì²« ë²ˆì§¸ ë°°í¬**
```bash
curl -X POST http://localhost:8005/deploy-from-file \
  -H "Content-Type: application/json" \
  -d '{"config_file": "test_scenario1.yaml"}'
```

**ì‹œë‚˜ë¦¬ì˜¤ 2: ë™ì¼í•œ ì„¤ì •ìœ¼ë¡œ ì¬ë°°í¬ (ì¬ì‚¬ìš© í…ŒìŠ¤íŠ¸)**
```bash
curl -X POST http://localhost:8005/deploy-from-file \
  -H "Content-Type: application/json" \
  -d '{"config_file": "test_scenario2.yaml"}'
```

**ì‹œë‚˜ë¦¬ì˜¤ 3: ë‹¤ë¥¸ ì„¤ì •ìœ¼ë¡œ ë°°í¬ (ìƒˆ ë°°í¬)**
```bash
curl -X POST http://localhost:8005/deploy-from-file \
  -H "Content-Type: application/json" \
  -d '{"config_file": "test_scenario3.yaml"}'
```

**ì‹œë‚˜ë¦¬ì˜¤ 4: GPU ë¦¬ì†ŒìŠ¤ ì¶©ëŒ í…ŒìŠ¤íŠ¸**
```bash
curl -X POST http://localhost:8005/deploy-from-file \
  -H "Content-Type: application/json" \
  -d '{"config_file": "test_gpu_scenario.yaml"}'
```

#### 3. ë°°í¬ ê´€ë¦¬ ëª…ë ¹ì–´

**í˜„ì¬ ë°°í¬ ëª©ë¡ í™•ì¸**
```bash
curl http://localhost:8005/deployments | jq '.'
```

**íŠ¹ì • ë°°í¬ ìƒíƒœ í™•ì¸**
```bash
curl http://localhost:8005/deployments/{deployment_id}/status | jq '.'
```

**ë°°í¬ ì¤‘ì§€**
```bash
curl -X DELETE http://localhost:8005/deployments/{deployment_id}
```

#### 4. ì§ì ‘ ì„¤ì •ìœ¼ë¡œ ë°°í¬
```bash
# CPU ê¸°ë°˜ í…ŒìŠ¤íŠ¸ ë°°í¬
curl -X POST http://localhost:8005/deploy \
  -H "Content-Type: application/json" \
  -d '{
    "config": {
      "model_name": "microsoft/DialoGPT-small",
      "gpu_memory_utilization": 0.1,
      "max_num_seqs": 16,
      "gpu_resource_type": "cpu",
      "gpu_resource_count": 0,
      "port": 8000
    }
  }'

# GPU ë¦¬ì†ŒìŠ¤ í…ŒìŠ¤íŠ¸ ë°°í¬
curl -X POST http://localhost:8005/deploy \
  -H "Content-Type: application/json" \
  -d '{
    "config": {
      "model_name": "microsoft/DialoGPT-small",
      "gpu_resource_type": "nvidia.com/gpu",
      "gpu_resource_count": 1,
      "port": 8001
    }
  }'

# MIG ë¦¬ì†ŒìŠ¤ í…ŒìŠ¤íŠ¸ ë°°í¬
curl -X POST http://localhost:8005/deploy \
  -H "Content-Type: application/json" \
  -d '{
    "config": {
      "model_name": "microsoft/DialoGPT-small",
      "gpu_resource_type": "nvidia.com/mig-3g.20gb",
      "gpu_resource_count": 1,
      "port": 8002
    }
  }'
```

#### 5. Kubernetes ë¦¬ì†ŒìŠ¤ ì§ì ‘ í™•ì¸
```bash
# vllm ë„¤ì„ìŠ¤í˜ì´ìŠ¤ì˜ ëª¨ë“  ë¦¬ì†ŒìŠ¤ í™•ì¸
kubectl get all -n vllm

# ë°°í¬ ìƒì„¸ ì •ë³´ í™•ì¸
kubectl describe deployments -n vllm

# Pod ë¡œê·¸ í™•ì¸
kubectl logs -l app=vllm -n vllm --tail=50

# ì´ë²¤íŠ¸ í™•ì¸
kubectl get events -n vllm --sort-by='.lastTimestamp'
```

#### 6. í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¶„ì„
```bash
# ë°°í¬ ë¹„êµ ê²°ê³¼ í™•ì¸ (jq í•„ìš”)
curl -s http://localhost:8005/deployments | jq '
  to_entries | map({
    deployment_id: .key,
    model: .value.config.model_name,
    gpu_resource: .value.config.gpu_resource_type,
    gpu_count: .value.config.gpu_resource_count,
    status: .value.status,
    created_at: .value.created_at
  })
'

# ê°„ë‹¨í•œ ë°°í¬ ìš”ì•½
curl -s http://localhost:8005/deployments | jq -r '
  to_entries[] | 
  "\(.key): \(.value.config.model_name) (\(.value.config.gpu_resource_type) x \(.value.config.gpu_resource_count)) - \(.value.status)"
'
```

### ğŸ” í…ŒìŠ¤íŠ¸ í¬ì¸íŠ¸

1. **ë°°í¬ ì¬ì‚¬ìš© í™•ì¸**: ë™ì¼í•œ ì„¤ì •ìœ¼ë¡œ ë°°í¬ ì‹œ ê¸°ì¡´ ë°°í¬ ì¬ì‚¬ìš© ì—¬ë¶€
2. **GPU ë¦¬ì†ŒìŠ¤ ì¶©ëŒ ê°ì§€**: ê°™ì€ GPU ë¦¬ì†ŒìŠ¤ íƒ€ì… ì‚¬ìš© ì‹œ ì¶©ëŒ ê°ì§€ ì—¬ë¶€
3. **MIG ë¦¬ì†ŒìŠ¤ êµ¬ë¶„**: ë‹¤ë¥¸ MIG ìŠ¬ë¼ì´ìŠ¤ ê°„ ë…ë¦½ì„± í™•ì¸
4. **ë°°í¬ ìƒíƒœ ì¶”ì **: ë°°í¬ ìƒì„±, ì‹¤í–‰, ì¤‘ì§€ ìƒíƒœ ë³€í™” ì¶”ì 
5. **ì—ëŸ¬ ì²˜ë¦¬**: ì˜ëª»ëœ ì„¤ì •ì´ë‚˜ ë¦¬ì†ŒìŠ¤ ë¶€ì¡± ì‹œ ì—ëŸ¬ ì²˜ë¦¬

### âš ï¸ MacBook í™˜ê²½ ì œí•œì‚¬í•­

1. **GPU ì§€ì› ì—†ìŒ**: MacBookì—ëŠ” NVIDIA GPUê°€ ì—†ìœ¼ë¯€ë¡œ ì‹¤ì œ GPU ì›Œí¬ë¡œë“œëŠ” ì‹¤í–‰ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
2. **vLLM ì´ë¯¸ì§€ í˜¸í™˜ì„±**: vLLM Docker ì´ë¯¸ì§€ê°€ ARM64 (Apple Silicon)ë¥¼ ì§€ì›í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
3. **ë¦¬ì†ŒìŠ¤ ì œí•œ**: ë¡œì»¬ Kubernetes í´ëŸ¬ìŠ¤í„°ëŠ” ì œí•œëœ ë¦¬ì†ŒìŠ¤ë¥¼ ê°€ì§‘ë‹ˆë‹¤.

### ğŸ”§ MacBook ì „ìš© ì„¤ì • ì¡°ì •

MacBookì—ì„œ í…ŒìŠ¤íŠ¸í•  ë•ŒëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì„¤ì •ì„ ì¡°ì •í•˜ì„¸ìš”:

```yaml
# configs/vllm_config_macos.yaml
model_name: "microsoft/DialoGPT-small"  # ë” ì‘ì€ ëª¨ë¸ ì‚¬ìš©
gpu_memory_utilization: 0.1  # ë‚®ì€ ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ 
max_num_seqs: 16  # ì ì€ ì‹œí€€ìŠ¤ ìˆ˜
tensor_parallel_size: 1
pipeline_parallel_size: 1
gpu_resource_type: "nvidia.com/gpu"  # í…ŒìŠ¤íŠ¸ìš© (ì‹¤ì œë¡œëŠ” í• ë‹¹ë˜ì§€ ì•ŠìŒ)
gpu_resource_count: 1
port: 8000
```

### ğŸ› MacBook í™˜ê²½ íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

#### vLLM ì´ë¯¸ì§€ í˜¸í™˜ì„± ë¬¸ì œ
```bash
# ARM64 í˜¸í™˜ ì´ë¯¸ì§€ ì‚¬ìš© ë˜ëŠ” ë¹Œë“œ
docker build --platform linux/amd64 -t vllm-test .
```

#### Kubernetes ì—°ê²° ë¬¸ì œ
```bash
# í´ëŸ¬ìŠ¤í„° ìƒíƒœ í™•ì¸
kubectl cluster-info
kubectl get nodes

# ë„¤ì„ìŠ¤í˜ì´ìŠ¤ í™•ì¸
kubectl get namespaces
kubectl create namespace vllm
```

#### MongoDB ì—°ê²° ë¬¸ì œ
```bash
# MongoDB ìƒíƒœ í™•ì¸
docker ps | grep mongo
# ë˜ëŠ”
brew services list | grep mongodb

# ì—°ê²° í…ŒìŠ¤íŠ¸
mongosh --host localhost:27017
```

## ğŸ“ API ì‚¬ìš©ë²•

### ê¸°ë³¸ ì„¤ì • íŒŒì¼ë¡œ vLLMì„ vllm ë„¤ì„ìŠ¤í˜ì´ìŠ¤ì— ë°°í¬

```bash
curl -X POST http://localhost:8005/deploy-default
```

### ì‚¬ìš©ì ì •ì˜ ì„¤ì •ìœ¼ë¡œ Kubernetes Pod ë°°í¬

```bash
curl -X POST http://localhost:8005/deploy \
  -H "Content-Type: application/json" \
  -d '{
    "config": {
      "model_name": "microsoft/DialoGPT-medium",
      "gpu_memory_utilization": 0.9,
      "max_num_seqs": 256,
      "block_size": 16,
      "port": 8000
    }
  }'
```

### ë°°í¬ ìƒíƒœ í™•ì¸

```bash
curl http://localhost:8005/deployments/{deployment_id}/status
```

### ë°°í¬ ì¤‘ì§€

```bash
curl -X DELETE http://localhost:8005/deployments/{deployment_id}
```

### ë°°í¬ ì •ë¦¬ (Cleanup)

vllm ë„¤ì„ìŠ¤í˜ì´ìŠ¤ì— ë°°í¬ëœ ëª¨ë“  ë¦¬ì†ŒìŠ¤ë¥¼ ì •ë¦¬í•˜ëŠ” ë°©ë²•:

```bash
# 1. ëª¨ë“  ë°°í¬ í™•ì¸
kubectl get deployments -n vllm
kubectl get services -n vllm
kubectl get pods -n vllm

# 2. ê°œë³„ ë°°í¬ ì‚­ì œ (API ì‚¬ìš©)
curl -X DELETE http://localhost:8005/deployments/{deployment_id}

# 3. ë˜ëŠ” Kubernetes ëª…ë ¹ì–´ë¡œ ì§ì ‘ ì‚­ì œ
kubectl delete deployment {deployment_name} -n vllm
kubectl delete service {service_name} -n vllm

# 4. ëª¨ë“  vLLM ë¦¬ì†ŒìŠ¤ í•œë²ˆì— ì‚­ì œ
kubectl delete deployments,services -l app=vllm -n vllm

# 5. vllm ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ì „ì²´ ì‚­ì œ (ì£¼ì˜: ëª¨ë“  ë°ì´í„° ì‚­ì œë¨)
kubectl delete namespace vllm

# 6. ì‹¤í–‰ ì¤‘ì¸ ë¡œì»¬ ì„œë¹„ìŠ¤ ì¤‘ì§€
pkill -f "python.*main.py"
```

### ë°°í¬ ìƒíƒœ ëª¨ë‹ˆí„°ë§

```bash
# Kubernetes ë¦¬ì†ŒìŠ¤ ìƒíƒœ í™•ì¸
kubectl get pods -n vllm -w
kubectl logs -f deployment/{deployment_name} -n vllm

# ë°°í¬ ìƒì„¸ ì •ë³´ í™•ì¸
kubectl describe deployment {deployment_name} -n vllm
kubectl describe pod {pod_name} -n vllm
```

## ğŸ”§ ì„¤ì • íŒŒì¼ í˜•ì‹

`configs/vllm_config.yaml` ì˜ˆì‹œ:

```yaml
model_name: "microsoft/DialoGPT-medium"
gpu_memory_utilization: 0.9
max_num_seqs: 256
block_size: 16
tensor_parallel_size: 1
pipeline_parallel_size: 1
trust_remote_code: false
dtype: "auto"
port: 8000
host: "0.0.0.0"
# GPU ë¦¬ì†ŒìŠ¤ ì„¤ì •
gpu_resource_type: "nvidia.com/gpu"
gpu_resource_count: 1
```

### GPU ë¦¬ì†ŒìŠ¤ íƒ€ì… ì˜ˆì‹œ

```yaml
# ì¼ë°˜ GPU ë¦¬ì†ŒìŠ¤
gpu_resource_type: "nvidia.com/gpu"
gpu_resource_count: 1

# MIG 3g.20gb ë¦¬ì†ŒìŠ¤
gpu_resource_type: "nvidia.com/mig-3g.20gb"
gpu_resource_count: 1

# MIG 4g.24gb ë¦¬ì†ŒìŠ¤
gpu_resource_type: "nvidia.com/mig-4g.24gb"
gpu_resource_count: 1

# ë‹¤ì¤‘ GPU ë¦¬ì†ŒìŠ¤
gpu_resource_type: "nvidia.com/gpu"
gpu_resource_count: 2
```

## ğŸ® GPU ë¦¬ì†ŒìŠ¤ ê´€ë¦¬

### ìë™ ì¶©ëŒ ê°ì§€ ë° í•´ê²°

ë°°í¬ ì‹œ ë‹¤ìŒê³¼ ê°™ì€ ë¡œì§ì´ ìë™ìœ¼ë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤:

1. **ê¸°ì¡´ ë°°í¬ í™•ì¸**: ë™ì¼í•œ ëª¨ë¸ê³¼ ì„¤ì •ìœ¼ë¡œ ì‹¤í–‰ ì¤‘ì¸ ë°°í¬ê°€ ìˆëŠ”ì§€ í™•ì¸
2. **ë°°í¬ ì¬ì‚¬ìš©**: ì™„ì „íˆ ì¼ì¹˜í•˜ëŠ” ì„¤ì •ì˜ ë°°í¬ê°€ ìˆìœ¼ë©´ ìƒˆë¡œ ë°°í¬í•˜ì§€ ì•Šê³  ê¸°ì¡´ ë°°í¬ ì¬ì‚¬ìš©
3. **GPU ë¦¬ì†ŒìŠ¤ ì¶©ëŒ ê°ì§€**: ê°™ì€ GPU ë¦¬ì†ŒìŠ¤ íƒ€ì…ì„ ì‚¬ìš©í•˜ëŠ” ë‹¤ë¥¸ ë°°í¬ê°€ ìˆëŠ”ì§€ í™•ì¸
4. **ê¸°ì¡´ ë°°í¬ ì •ë¦¬**: ì¶©ëŒí•˜ëŠ” ë°°í¬ê°€ ìˆìœ¼ë©´ ìë™ìœ¼ë¡œ ì¤‘ì§€
5. **ìƒˆ ë°°í¬ ìƒì„±**: ì¶©ëŒì´ í•´ê²°ëœ í›„ ìƒˆë¡œìš´ ë°°í¬ ìƒì„±

### ì¶©ëŒ ê°ì§€ ê·œì¹™

- **ì¼ë°˜ GPU**: `nvidia.com/gpu` ë¦¬ì†ŒìŠ¤ë¥¼ ì‚¬ìš©í•˜ëŠ” ëª¨ë“  ë°°í¬ëŠ” ì„œë¡œ ì¶©ëŒ
- **MIG GPU**: ê°™ì€ MIG ìŠ¬ë¼ì´ìŠ¤ íƒ€ì… (ì˜ˆ: `3g.20gb`, `4g.24gb`)ì„ ì‚¬ìš©í•˜ëŠ” ë°°í¬ëŠ” ì„œë¡œ ì¶©ëŒ
- **ë‹¤ë¥¸ ë¦¬ì†ŒìŠ¤ íƒ€ì…**: ì„œë¡œ ë‹¤ë¥¸ GPU ë¦¬ì†ŒìŠ¤ íƒ€ì…ì€ ì¶©ëŒí•˜ì§€ ì•ŠìŒ

### ë°°í¬ ì¬ì‚¬ìš© ì¡°ê±´

ë‹¤ìŒ ì„¤ì •ì´ ëª¨ë‘ ì¼ì¹˜í•´ì•¼ ë°°í¬ê°€ ì¬ì‚¬ìš©ë©ë‹ˆë‹¤:

- `model_name`: ëª¨ë¸ ì´ë¦„
- `gpu_memory_utilization`: GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ 
- `max_num_seqs`: ìµœëŒ€ ì‹œí€€ìŠ¤ ìˆ˜
- `block_size`: ë¸”ë¡ í¬ê¸°
- `tensor_parallel_size`: í…ì„œ ë³‘ë ¬ í¬ê¸°
- `pipeline_parallel_size`: íŒŒì´í”„ë¼ì¸ ë³‘ë ¬ í¬ê¸°
- `trust_remote_code`: ì›ê²© ì½”ë“œ ì‹ ë¢° ì—¬ë¶€
- `dtype`: ë°ì´í„° íƒ€ì…
- `max_model_len`: ìµœëŒ€ ëª¨ë¸ ê¸¸ì´
- `quantization`: ì–‘ìí™” ë°©ë²•
- `served_model_name`: ì„œë¹™ ëª¨ë¸ ì´ë¦„
- `gpu_resource_type`: GPU ë¦¬ì†ŒìŠ¤ íƒ€ì…
- `gpu_resource_count`: GPU ë¦¬ì†ŒìŠ¤ ê°œìˆ˜
- `additional_args`: ì¶”ê°€ ì¸ìˆ˜

## ğŸ³ Docker ì‹¤í–‰

```bash
# ì´ë¯¸ì§€ ë¹Œë“œ
docker build -t benchmark-vllm .

# ì»¨í…Œì´ë„ˆ ì‹¤í–‰
docker run -p 8005:8005 -e MONGO_URL="your_mongo_url" benchmark-vllm
```

## â˜¸ï¸ Kubernetes ë°°í¬

```bash
kubectl apply -f benchmark-vllm-deployment.yaml
```

## ğŸ“Š ëª¨ë‹ˆí„°ë§

- Health Check: `GET /health`
- System Status: `GET /status`
- API ë¬¸ì„œ: `http://localhost:8005/docs`

## ğŸ§¹ íŠ¸ëŸ¬ë¸”ìŠˆíŒ… ë° ì •ë¦¬

### í˜„ì¬ ë°°í¬ëœ vLLM í™•ì¸
```bash
# APIë¡œ ë°°í¬ ëª©ë¡ í™•ì¸
curl http://localhost:8005/deployments

# Kubernetesë¡œ ì§ì ‘ í™•ì¸
kubectl get pods -n vllm
kubectl get services -n vllm
```

### ë¬¸ì œê°€ ìˆëŠ” ë°°í¬ ì •ë¦¬
```bash
# íŠ¹ì • ë°°í¬ ì‚­ì œ
curl -X DELETE http://localhost:8005/deployments/{deployment_id}

# ì‘ë‹µí•˜ì§€ ì•ŠëŠ” Pod ê°•ì œ ì‚­ì œ
kubectl delete pod {pod_name} -n vllm --force --grace-period=0

# ëª¨ë“  vLLM ë¦¬ì†ŒìŠ¤ ì •ë¦¬
kubectl delete deployments,services,pods -l app=vllm -n vllm
```

### ì™„ì „ ì´ˆê¸°í™”
```bash
# 1. ë¡œì»¬ ì„œë¹„ìŠ¤ ì¤‘ì§€
pkill -f "python.*main.py"

# 2. vllm ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ì‚­ì œ
kubectl delete namespace vllm

# 3. ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ì¬ìƒì„±
kubectl create namespace vllm

# 4. ì„œë¹„ìŠ¤ ì¬ì‹œì‘
python main.py
```